---
layout: post
toc: true
title: "智能随想-关于AGI的一些基本观点和假设"
categories: private
tags: [section1]
author:
  - David
---

从3年前开始，笔者长期对于人工智能模型的发展前景，保持前所未有的乐观，甚至认为AGI可能就在未来的十几年就能产生或者出现，3年过去了，从去年开始出现的大规模
模型的功能，似乎也进一步加强了我的这种认识，但随着深入理解现在的模型CLIP，DIFFUSION，BERT，GPT等，笔者觉得这个认识可能需要退一步，必须保持清醒。

首先第一点，我们对于知识的认识，好像还是从模型的涉及角度跳开了，或者说不被重视，所以将模型溶解更多的数据，作为了当前模型研究的方向。例如，文本转图片，
图片转文本，文本转文本等，这些多模态的任务中，或者内容转换的模型，都还是在10年前的表示学习的认知上进行，大家都认为一段128维或者更高维度的语义向量，可
以表达整个任务所需要的全部信息，全过程都是"编码-解码"的逻辑架构，但可以显而易见的是这种架构做到的是表象的。

笔者觉得Hinton在17年capsule的工作，就是在强调解构知识，end2end的训练模型方式，对数据进行了重新编码，但这样的结果，并不符合我们对于学习知识的理解，
最直接的表现就是这样的系统是封闭式的，但是我们对于知识的学习，是在一个逻辑自洽的知识体系中开放式的。简单来说，这样的模型设计，完全不具备可扩展性，例如：
当前非常火爆的生成模型，可以想象，在模型的内容，高维度混杂语义编码，可以实现对于自然语言中各种不同文字token间关联的信息进行概率评估，从而反馈出在输入
一段序列后，如何一步一步输出接下来关联的文字，但是模型无法获得概念性的理解或者抽象。这点最直接的表现就是他对于概念性的描述，只存在于训练语料中是否包含了
对应的文字关系语料，但并不会从自洽的逻辑概念知识中，通过不断反溃-推断这样的机制，来完善这样的概念。再比如：ChatGPT这样的生成模型，可以通过思维链的启
发式prompt的方式来训练模型去解决简单的逻辑任务，但从笔者的多次反复的评估中，即便是在优化prompt的方式中，依旧需要通过输入较为逻辑封闭的prompt来获得
模型输出正确的结果，并且在明显给出错误的prompt的前提下，模型会顺着错误逻辑的方向进行回答，而主动放弃了原本自己已经回答正确的结果。从这些角度上来讲，
缺乏自洽逻辑说明当前模型还只是一个encoder-decoder的工具，当然，这样的工具具有强大的地方，要溶解如此巨量的数据，依然是一件非常不容易的事情。

具备逻辑自洽的推理能力是对于开放式知识进行抽象的第一步，笔者理解这依然是需要基本的知识体系，但AGI的发展方式，可能需要从这个角度来思考如何通过当前神经
网络模型对于信息统一式编码的方式，设计可以进行自洽逻辑推理的学习。但是这似乎也并不容易，因为在逻辑里边，自洽的方式可能并不是唯一，这个笔者觉得是第一个
可能比较不太能被逾越的壁垒，因为如果我们的考虑是构建一个足够强大的模型，让它可以解释或者学习所有事情，那么这样的模型可能并不能实现，这对于一个个体的
人脑来说就是一件非常困难的事情，所以通过大规模的人造数据来进行学习的模型，可能存在知识对立的情况，这可能导致模型无法进行有效的知识解构和学习，当然这已
经假设模型可以进行统一式编码后的概念抽象，同时进行推导-反馈学习了。所以我们现在累积的人造数据，是否真实，是否能够作为模型的学习原料，可能需要更深程度地
审视。

第二点，对于知识来说，另外一个笔者没有想明白的问题，即：假设模型可以有效的进行学逻辑自洽地学习，并抽象化理解概念，我们如何帮助模型对于虚假概念进行学习
和剔除。这可能是一个更加复杂的命题。简单来说，就是当模型面对小说或者是虚构的图像进行学习的时候，需要购建起更高级的概念体系，但这样的概念应当一部分被描
述为虚构的，另外一部分应当被描述为真实的，之所以是虚构的这是小说和图像内容创作的基本假设，同时是真实的，因为这些内容本身上能够被人理解，内在的逻辑依然，
是自洽的，所以应当具备某种内在的真实性。但是对于模型来说，即便可以理解到其数据层面内在逻辑的自洽，也需要"自主"或者被人工干预性的，告知模型的学习-这样的
数据本身是虚假的。这样的学习机制，对于人可能是简单的，笔者认为其主要原因在于，人脑不仅可以通过基础知识体系，来对新概念和知识进行逻辑自洽的反馈-推断学习，
更加重要的是，人可以结合真实的实践活动对目标知识进行检验，从而获得真实的反馈，虽然目前的强化学习已经在人型机器人的活动、自动驾驶等多种场景下，实现了部
分有条件的与环境接触反馈的学习，但是对于复杂概念性的验证实践，在笔者认为目前看起来，这样的事情显得十分科幻。从这个角度出发，笔者对于可以考虑通过AGI来
实现某些科学方向的短期爆炸性突破的预期，自然也是显得想的简单了，或者这样的思考，需要特定的科学方向具备一定的可靠研究范式，例如蛋白质的空间结构预测等。

笔者认为对于AGI来说，虽然目前看起来，从知识以及自洽的逻辑学习来说，可能是存在较强的认知限制，但是退一步来说，即便无法构建出知晓且符合全部甚至部分人理解
认知的超级智能大脑，但是似乎这并不影响现在的模型实现进行简单的文字理解和输出，如果希望期待它能够解决更加复杂的问题，我们可能还是需要构建有限知识边界的
人工智能模型。
