---
layout: post
toc: true
title: "智能随想-关于AGI的一些浅显观点和假设"
categories: 人工智能
tags: [AGI, 知识体系, 逻辑自洽, 编码]
author:

- David

---

从3年前开始，笔者对于人工智能模型的长期发展前景，保持前所未有的乐观，甚至认为AGI可能就在未来的十几年就能产生或者出现。现在3年过去了，当下大规模
模型（LLM）的性能和效果，似乎也进一步加强了笔者的这种认识，但随着深入诸如：CLIP，DIFFUSION，BERT，GPT等模型的理解，笔者突然认识到必须要保持足够的清醒。

首先不可否认，LLM的出现对于当下的人工智能的发展无疑又是打了一针强心剂，ChatGPT、Stable Diffusion等模型的效果，远远超越了大多数人固有的认识，改变了很多人对于人工智能的认知，大量关于AIGC的话题和应用也层出不穷，然而非凡的效果往往是吸引人们第一眼美丽的外貌，然而对于模型架构、模型效果的分析和理解，才是进一步能够提升当前技术方案的基石。然而从这个角度出发，笔者认为我们应该更加理性地来看待LLM，它的发展前景或者技术演进的方向，似乎跟我们的向往、预期并不是那么吻合。

首先第一点，我们对于知识的认识，好像还是从模型的设计环节跳开了，或者说不被重视。而是以模型"溶解"更多的数据，挖掘隐层语义向量的特定关系，作为了当前模型研究的主要方向。例如，文本转图片，图片转文本，文本转文本等，对这些多模态或者内容转换的任务设计问题解决范式的时候，认为一段128维或者更高维度的语义向量，经过了end2end的模型拟合，可以很容易地将标注信息融合在高维语义向量中，甚至可以表达整个任务所需要的全部信息，从而以统一信息表达标准的语义向量来实现、完成目标任务。但“学习”的范式依旧在10年前的“表示学习”的认知上进行。笔者认为这样的模型设计范式是飞跃式的、划时代的，但并不代表能够实现人类级别的小样本、复杂的学习。同时全过程"编码-解码"的逻辑架构，显而易见解决的问题都还是表象的。

虽然end2end的训练模型方式，对数据进行了重新编码在目前的学习任务中效果是最好的，但同时也是黑盒式的，是封闭式的。相对比下，我们对于知识的学习，是在一个逻辑自洽的知识体系中开放式进行的。 熟悉的读者可能了解，目前所有大模型的构建，几乎都是结合巨量数据的训练模式来实现的。由于数据的体量巨大，间接导致无法进行进一步人为有效的筛选，所以是全量数据一起进行训练的。模型可以学习到信息统一式编码的方式，例如文字之间的词和字的位置、概率关系，图像中像素块的位置，概率关系等。这一预训练模式被认为为模型前置构建了一套可以输入、输出跟人进行信息交流的基础模型。进一步为了使模型更加符合于某一特定状态和领域的输出，我们再通过设计下游模型，结合少量的有监督数据对模型进行微调，改变预训练模型本身内在的编码方式，使其输出满足预期，例如：当前非常火热的ChatGPT模型，以通过2轮人工筛选的prompt，分别对模型进行了生成文本序列的优化及标准回答模式的优化等。但这样的过程，并不符合我们对于学习的理解，最直接的表现就是这样的学习，构建的知识表达体系是封闭式的，一旦没有校验的标签，模型就完全丧失了判断能力，若需要获得海量的标签学习，以覆盖知识盲区，这又是不可能或成本极高的。在ChatGPT模型生成内容的过程中，高维度混杂语义编码，实现对于自然语言中各种不同文字token间关联的信息进行概率评估，从而在输入一段序列后输出接下来关联的文字。模型似乎并没有获得概念性的理解或者抽象，最直接的表现就是模型对于概念性的描述，只取决于训练语料中是否包含了对应的文字关系数据。虽然模型开发者试图通过让模型学习不同回答模式的训练数据，用prompt和标准模式的答案的配对数据，引导来优化模型对于问题的回答，但它依然不会从自洽的逻辑知识体系中，通过不断反溃-推断这样的机制，来不断完善或者加深理解同一个概念。再比如：ChatGPT这样的生成模型，可以通过思维链的启发式prompt的方式来训练模型去解决简单的逻辑任务，但从笔者的多次反复的评估中，即便是在优化prompt的方式中，依旧需要通过输入极为逻辑封闭的prompt来获得模型输出正确的结果，并且在明显给出错误的prompt的前提下，模型会顺着错误逻辑的方向进行回答，而主动放弃了原本自己已经回答正确的结果。从这些角度上来讲，缺乏自洽逻辑说明当前模型还只是一个encoder-decoder的工具，虽然这样的工具已经很强大。

![image]({{site.url}}/assets/source/20230203/0_0.jpg)
![image]({{site.url}}/assets/source/20230203/0_1.jpg)


![image]({{site.url}}/assets/source/20230203/1_0.jpg)
![image]({{site.url}}/assets/source/20230203/1_1.jpg)
![image]({{site.url}}/assets/source/20230203/1_2.jpg)
![image]({{site.url}}/assets/source/20230203/1_3.jpg)
![image]({{site.url}}/assets/source/20230203/1_4.jpg)
![image]({{site.url}}/assets/source/20230203/1_5.jpg)

<sub>_上图所示是笔者对chatgpt提出的2个关于逻辑方面的问题。可以看出来chatgpt是在尝试做分析，这样的分析是可以通过思维链（CoT）的prompt语料进行对应
的训练学习，但模型依然是缺少对于自己分析结果的判断能力。_</sub>

具备逻辑自洽的推理能力是对于开放式知识进行抽象的第一步，然而即便是设计可以进行自洽逻辑推理的学习策略，我们原始对于构造出来一套知晓万事的人工智能模型的预想也可能是破灭的。首先，训练的数据本身可能存在着多种结果导向完全矛盾的自洽逻辑，这是笔者觉得进一步提升AGI模型面临的第一个不太能被逾越的壁垒。对于个体的人来说，矛盾的认知，对于知识体系本身就是一种破坏性的冲击，很难让个体同时接受两种完全矛盾的概念。这种现象在现实种经常可以见到，例如：部分老人家总是会认为自己能通过熟人讲解做个网站，就在短视频平台或者电商平台中进行电子商务，但绝大多数年轻人，会更加清醒地认识这是骗子的把戏，可是这两种对于电子商务的认识就截然不同，不会有人同时认为自己被骗了，可是依然可以赚钱。基于这样的理解，如果我们的考虑是构建一个足够强大的模型，让它可以解释或者学习所有事情，那么这样的模型可能并不能实现。两种相悖的概念并存于认知、知识里边，必定会造成支撑其概念的底层逻辑的改变，这种改变对于知识编码的方式来说，可能是非常巨大的，甚至导致输出是不确定的，如同对于人类知识体系的破坏。那么我们可能做到的，就是围绕一套合理的逻辑对数据进行选择，对模型进行训练，让模型可以输出符合某一种自洽逻辑的输出，显然这天然就否定了我们可以构建一个什么都知晓的人工智能模型的可能性。那么我们现在累积的人造数据，是否“真实”，是否能够作为模型的学习原料，可能需要更深度和全面地审视。

另外一个笔者认为不太能逾越的壁垒，是一旦模型遇到新抽象化概念的时候，我们如何帮助模型对新的虚假概念进行甄别，这可能是一个更加复杂的命题。在新的知识和概念的发现的过程中，人不仅可以通过基础知识体系，来对新概念和知识进行逻辑自洽的学习，更加重要的是，可以主动结合真实的实践活动对目标知识进行检验，从而获得真实的反馈，虽然目前的强化学习已经在人型机器人的活动、自动驾驶等多种场景下，实现了部分有条件的与环境接触反馈的学习，但是对于复杂概念性的验证实践不太可能实现，我们可能可以在电影中看到，机器人帮助科学家发现了某一种新的知识，同时还通过自己实验的方式，对知识进行了验证，但显然从当前的技术可实现性的角度考虑，这样的事情显得十分科幻。因此对于考虑通过AGI来实现某些科学方向的短期爆炸性突破的预期，自然也是想的简单了，或者这样的思考需要特定的场景下，例如：某学科方向具备一定的可靠研究范式，例如蛋白质的空间结构预测等。笔者认为AI在新的概念和知识发现的方向，未来最可能的方式依然是帮助人类缩小探索新知识的范围。

总结，笔者认为对于AGI来说，虽然目前看起来，从知识以及自洽的逻辑学习来说，可能是存在较强的认知限制，但是退一步来说，即便无法构建出知晓且符合全部甚至部分人理解认知的超级智能大脑，但是似乎这并不影响现在的模型实现进行简单的文字理解和输出，如果希望期待它能够解决更加复杂的问题，我们还是需要构建有限知识边界的人工智能模型，或者从底层来思考更加本质的问题。
